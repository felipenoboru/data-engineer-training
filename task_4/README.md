# ğŸš€ InstruÃ§Ãµes para Projeto Airflow + DBT (Docker Compose)

## ğŸ“‘ Ãndice
1. [Objetivo](#objetivo)  
2. [PrÃ©-requisitos](#prÃ©-requisitos)  
3. [Estrutura de Pastas](#estrutura-de-pastas)  
4. [Subindo o Airflow com Docker Compose](#subindo-o-airflow-com-docker-compose)  
5. [Criando uma DAG de Exemplo](#criando-uma-dag-de-exemplo)  
6. [IntegraÃ§Ã£o Airflow + DBT](#integraÃ§Ã£o-airflow--dbt)  
7. [VerificaÃ§Ã£o e Monitoramento](#verificaÃ§Ã£o-e-monitoramento)  

---

## ğŸ“Œ Objetivo
Este exercÃ­cio tem o objetivo de validar seus conhecimentos em **Apache Airflow** e **DBT**.  
VocÃª irÃ¡:
1. Subir uma instÃ¢ncia **Airflow** local usando **Docker Compose**.  
2. Montar volumes para que a pasta `dags/` no host fique sincronizada com o contÃªiner.  
3. Criar uma **DAG** simples (ETL dummy) e executar.  
4. Desafio extra: orquestrar a execuÃ§Ã£o dos seus modelos **DBT** dentro do Airflow, analisando agendamentos, logs e dependÃªncias.

---

## âœ… PrÃ©-requisitos
- Docker Desktop ou Docker Engine + Docker Compose instalados.  
- Python 3.8+ (apenas se for testar DBT fora do contÃªiner).  
- Projeto DBT configurado (pode ser o projeto dos exercÃ­cios anteriores).  

---

## ğŸ“‚ Estrutura de Pastas

```
task_4/
â”œâ”€â”€ airflow/                # DiretÃ³rio raiz do Airflow
â”‚   â”œâ”€â”€ dags/               # <volume> DefiniÃ§Ãµes das DAGs
â”‚   â”œâ”€â”€ plugins/            # <volume> Operadores / Hooks customizados
â”‚   â”œâ”€â”€ logs/               # <volume> Logs gerados pelas tasks
â”‚   â”œâ”€â”€ config/             # Arquivos extras de configuraÃ§Ã£o (opcional)
â”‚   â”œâ”€â”€ scripts/            # Scripts utilitÃ¡rios (pode conter shell ou python)
â”‚   â””â”€â”€ docker-compose.yml  # Stack Airflow (Webserver, Scheduler, DB, etc.)
â””â”€â”€ README.md               # Este documento
```

DescriÃ§Ã£o rÃ¡pida:  
- `dags/`: onde vocÃª colocarÃ¡ os arquivos `.py` das DAGs.  
- `plugins/`: permite adicionar operadores/hooks. Ãštil para integrar DBT ou S3, por exemplo.  
- `logs/`: volume para persistir logs fora do contÃªiner.  
- `config/`: use para um `airflow.cfg` customizado ou variÃ¡veis.  
- `scripts/`: scripts de inicializaÃ§Ã£o ou helpers (por exemplo, instalar dependÃªncias extras).  
- `docker-compose.yml`: define serviÃ§os `webserver`, `scheduler`, `postgres`, entre outros.

---

## ğŸ³ Subindo o Airflow com Docker Compose

Antes de iniciar, exporte a variÃ¡vel `AIRFLOW_UID` (evita problemas de permissÃ£o de pastas
no host Linux/macOS):

```bash
export AIRFLOW_UID=$(id -u)
```
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.10.5/docker-compose.yaml'

Dentro de `task_4/airflow/` execute:

```bash
# 1 ï¸âƒ£  Passo Ãºnico de inicializaÃ§Ã£o (cria metadados, conexÃµes padrÃµes etc.)
docker compose up airflow-init

# 2 ï¸âƒ£  Suba todo o stack em modo detached
docker compose up -d           # ou  docker compose up   (modo foreground)
```

Dicas Ãºteis (retiradas do guia oficial):
- Para **parar** os serviÃ§os:  
  ```bash
  docker compose down
  ```
- Para parar **e** apagar volumes (reiniciar do zero):  
  ```bash
  docker compose down --volumes
  ```
- Desabilite DAGs de exemplo definindo a variÃ¡vel:
  ```bash
  AIRFLOW__CORE__LOAD_EXAMPLES=false
  ```
  (adicione no `docker-compose.yml` ou como `export`).
- Ver logs combinados:
  ```bash
  docker compose logs -f
  ```
- Atualize uma DAG: salve o arquivo em `airflow/dags/`; o scheduler recarrega a cada ~30 s.

Para maiores detalhes consulte:  
https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html

Acesse o Airflow UI em `http://localhost:8080`  
UsuÃ¡rio e senha padrÃµes: `airflow / airflow` (override via variÃ¡veis no compose).

---

## ğŸ—ï¸ Criando uma DAG de Exemplo

Crie o arquivo `airflow/dags/hello_dbt.py`:

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

default_args = {"owner": "airflow", "retries": 0}

with DAG(
    dag_id="hello_dbt",
    start_date=datetime(2025, 1, 1),
    schedule_interval=None,
    catchup=False,
    default_args=default_args,
    tags=["demo"],
) as dag:

    say_hello = BashOperator(
        task_id="say_hello",
        bash_command="echo 'Hello Airflow!'"
    )
```

Salve; o Airflow detectarÃ¡ a nova DAG automaticamente.

---

## ğŸ”„ IntegraÃ§Ã£o Airflow + DBT

1. Copie (ou monte) seu projeto DBT em `airflow/scripts/dbt_project/`.  
2. Adicione uma task Bash no arquivo da DAG:

```python
run_dbt = BashOperator(
    task_id="run_dbt",
    bash_command="""
      cd /opt/airflow/scripts/dbt_project &&
      dbt deps &&
      dbt run --profiles-dir /opt/airflow/scripts/dbt_project
    """,
    env={"AWS_PROFILE": "mobiis", "AWS_DEFAULT_REGION": "us-east-2"},
)

say_hello >> run_dbt
```

3. Adicione `dbt-athena-community` ao `Dockerfile` custom ou instale dentro do contÃªiner usando um script em `scripts/`.

---

## ğŸ” VerificaÃ§Ã£o e Monitoramento

- **Logs**: clique na task dentro do Airflow UI e veja `Log`.  
- **Agendamento**: altere `schedule_interval` para `'@daily'` ou cron-like.  
- **DependÃªncias**: visualize o *Graph View* para verificar a ordem de execuÃ§Ã£o.  
- **DBT Docs**: rode `dbt docs generate` dentro da task e sirva em outra porta, se desejar.

> Pronto! Agora vocÃª tem um ambiente Airflow local, com DAGs versionadas em pasta, integraÃ§Ã£o DBT e visibilidade completa via UI e logs.